{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from typing import Callable, Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1.0,\n",
       " {'X': array([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [5, 6, 7]]),\n",
       "  'N': array([[ 8],\n",
       "         [26],\n",
       "         [38]]),\n",
       "  'P': array([[10],\n",
       "         [28],\n",
       "         [40]]),\n",
       "  'y': array([[11],\n",
       "         [27],\n",
       "         [41]])})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Linear Regression\n",
    "def forward_linear_regression(X_batch: ndarray, y_batch: ndarray, weights: Dict[str, ndarray]) -> Tuple[float, Dict[str, ndarray]]:\n",
    "    assert X_batch.shape[0] == y_batch.shape[0], \"X and y batches number of rows do not match\"\n",
    "    assert X_batch.shape[1] == weights[\"W\"].shape[0], \"X batch number of columns and weights number of rows do not match\"\n",
    "    assert weights[\"B\"].shape[0] == weights[\"W\"].shape[1] == 1, \"B number of rows and W number of columns different from 1\"\n",
    "\n",
    "    N = np.dot(X_batch, weights[\"W\"])\n",
    "    P = N + weights[\"B\"]\n",
    "    loss = np.mean(np.power(y_batch - P, 2))\n",
    "\n",
    "    # save the information computed on the forward pass\n",
    "    forward_info: Dict[str, ndarray] = {}\n",
    "    forward_info['X'] = X_batch\n",
    "    forward_info['N'] = N\n",
    "    forward_info['P'] = P\n",
    "    forward_info['y'] = y_batch\n",
    "\n",
    "    return loss, forward_info\n",
    "\n",
    "X_batch = np.array([\n",
    "    [0, 1, 2], # X1\n",
    "    [3, 4, 5], # X2\n",
    "    [5, 6, 7], # X3\n",
    "])\n",
    "\n",
    "weights = {\n",
    "    \"B\": np.array([2]), # will be broadcasted (?) as if np.array([[2], [2], [2]])\n",
    "    \"W\": np.array([\n",
    "        # W1\n",
    "        [1], \n",
    "        [2], \n",
    "        [3],\n",
    "    ]),\n",
    "}\n",
    "\n",
    "y_batch = np.array([\n",
    "    [11],\n",
    "    [27],\n",
    "    [41],\n",
    "])\n",
    "\n",
    "loss, forward_info = forward_linear_regression(X_batch, y_batch, weights)\n",
    "loss, forward_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'dLdP': array([[-2],\n",
       "        [ 2],\n",
       "        [-2]]),\n",
       " 'dPdN': array([[1],\n",
       "        [1],\n",
       "        [1]]),\n",
       " 'dLdN': array([[-2],\n",
       "        [ 2],\n",
       "        [-2]]),\n",
       " 'dNdW': array([[0, 3, 5],\n",
       "        [1, 4, 6],\n",
       "        [2, 5, 7]]),\n",
       " 'W': array([[-4],\n",
       "        [-6],\n",
       "        [-8]]),\n",
       " 'dPdB': array([1]),\n",
       " 'dLdB': array([[-2],\n",
       "        [ 2],\n",
       "        [-2]]),\n",
       " 'B': array([-2])}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Calculating the Gradients\n",
    "def loss_gradients(forward_info: Dict[str, ndarray], weights: Dict[str, ndarray]) -> Dict[str, ndarray]:\n",
    "    dLdP = -2 * (forward_info[\"y\"] - forward_info[\"P\"])\n",
    "    dPdN = np.ones_like(forward_info[\"N\"])\n",
    "    dLdN = dLdP * dPdN\n",
    "    dNdW = np.transpose(forward_info[\"X\"])\n",
    "    dLdW = np.dot(dNdW, dLdN)\n",
    "\n",
    "    dPdB = np.ones_like(weights[\"B\"])\n",
    "    dLdB = dLdP * dPdB\n",
    "    B = dLdB.sum(axis=0) # axis sum due to 1-dim B (?)\n",
    "\n",
    "    # intermdiate results added to B and W for debugging purposes\n",
    "    loss_gradients = {\n",
    "        \"dLdP\": dLdP,\n",
    "        \"dPdN\": dPdN,\n",
    "        \"dLdN\": dLdN,\n",
    "        \"dNdW\": dNdW,\n",
    "        \"W\": dLdW,\n",
    "        \"dPdB\": dPdB,\n",
    "        \"dLdB\": dLdB,\n",
    "        \"B\": B, \n",
    "    }\n",
    "    return loss_gradients\n",
    "\n",
    "loss_gradients(forward_info, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}