{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from typing import Callable, Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1.0,\n",
       " {'X': array([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [5, 6, 7]]),\n",
       "  'N': array([[ 8],\n",
       "         [26],\n",
       "         [38]]),\n",
       "  'P': array([[10],\n",
       "         [28],\n",
       "         [40]]),\n",
       "  'y': array([[11],\n",
       "         [27],\n",
       "         [41]])})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Linear Regression\n",
    "def forward_linear_regression(X_batch: ndarray, y_batch: ndarray, weights: Dict[str, ndarray]) -> Tuple[float, Dict[str, ndarray]]:\n",
    "    assert X_batch.shape[0] == y_batch.shape[0], \"X and y batches number of rows do not match\"\n",
    "    assert X_batch.shape[1] == weights[\"W\"].shape[0], \"X batch number of columns and weights number of rows do not match\"\n",
    "    assert weights[\"B\"].shape[0] == weights[\"W\"].shape[1] == 1, \"B number of rows and W number of columns different from 1\"\n",
    "\n",
    "    N = np.dot(X_batch, weights[\"W\"])\n",
    "    P = N + weights[\"B\"]\n",
    "    loss = np.mean(np.power(y_batch - P, 2))\n",
    "\n",
    "    # save the information computed on the forward pass\n",
    "    forward_info = {\n",
    "        \"X\": X_batch,\n",
    "        \"N\": N,\n",
    "        \"P\": P,\n",
    "        \"y\": y_batch,\n",
    "    }\n",
    "\n",
    "    return loss, forward_info\n",
    "\n",
    "X_batch = np.array([\n",
    "    [0, 1, 2], # X1\n",
    "    [3, 4, 5], # X2\n",
    "    [5, 6, 7], # X3\n",
    "])\n",
    "\n",
    "weights = {\n",
    "    # \"B\": np.array([[2], [2], [2]]),\n",
    "    \"B\": np.array([[2]]), # will be broadcasted (?) as if np.array([[2], [2], [2]])\n",
    "    # \"B\": np.array([2]), # will be broadcasted (?) as if np.array([[2], [2], [2]])\n",
    "    \"W\": np.array([\n",
    "        # W1\n",
    "        [1], \n",
    "        [2], \n",
    "        [3],\n",
    "    ]),\n",
    "}\n",
    "\n",
    "y_batch = np.array([\n",
    "    [11],\n",
    "    [27],\n",
    "    [41],\n",
    "])\n",
    "\n",
    "loss, forward_info = forward_linear_regression(X_batch, y_batch, weights)\n",
    "loss, forward_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'dLdP': array([[-2],\n",
       "        [ 2],\n",
       "        [-2]]),\n",
       " 'dPdN': array([[1],\n",
       "        [1],\n",
       "        [1]]),\n",
       " 'dLdN': array([[-2],\n",
       "        [ 2],\n",
       "        [-2]]),\n",
       " 'dNdW': array([[0, 3, 5],\n",
       "        [1, 4, 6],\n",
       "        [2, 5, 7]]),\n",
       " 'W': array([[-4],\n",
       "        [-6],\n",
       "        [-8]]),\n",
       " 'dPdB': array([[1]]),\n",
       " 'dLdB': array([[-2],\n",
       "        [ 2],\n",
       "        [-2]]),\n",
       " 'B': array([-2])}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Calculating the Gradients\n",
    "def loss_gradients(forward_info: Dict[str, ndarray], weights: Dict[str, ndarray]) -> Dict[str, ndarray]:\n",
    "    dLdP = -2 * (forward_info[\"y\"] - forward_info[\"P\"])\n",
    "    dPdN = np.ones_like(forward_info[\"N\"])\n",
    "    dLdN = dLdP * dPdN\n",
    "    dNdW = np.transpose(forward_info[\"X\"])\n",
    "    dLdW = np.dot(dNdW, dLdN)\n",
    "\n",
    "    dPdB = np.ones_like(weights[\"B\"])\n",
    "    dLdB = dLdP * dPdB\n",
    "    B = dLdB.sum(axis=0) # axis sum due to 1-dim B (?)\n",
    "\n",
    "    # intermdiate results added to B and W for debugging purposes\n",
    "    loss_gradients = {\n",
    "        \"dLdP\": dLdP,\n",
    "        \"dPdN\": dPdN,\n",
    "        \"dLdN\": dLdN,\n",
    "        \"dNdW\": dNdW,\n",
    "        \"W\": dLdW,\n",
    "        \"dPdB\": dPdB,\n",
    "        \"dLdB\": dLdB,\n",
    "        \"B\": B, \n",
    "    }\n",
    "    return loss_gradients\n",
    "\n",
    "loss_gradients(forward_info, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'W': array([[ 0.28766053],\n",
       "        [ 1.31596113],\n",
       "        [-0.53786283]]),\n",
       " 'B': array([[-0.38904971]])}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Using These Gradients to Train the Model\n",
    "def init_weights(n_in: int) -> Dict[str, ndarray]:\n",
    "    weights = {\n",
    "    \"W\": np.random.randn(n_in, 1),\n",
    "    \"B\": np.random.randn(1, 1),\n",
    "    }\n",
    "    return weights\n",
    "\n",
    "init_weights(3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [5, 6, 7]]),\n",
       " array([[11],\n",
       "        [27],\n",
       "        [41]]),\n",
       " array([0, 1, 2]))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "def permute_data(X: ndarray, y: ndarray):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm], perm\n",
    "\n",
    "X = np.array([\n",
    "    [0, 1, 2], # X1\n",
    "    [3, 4, 5], # X2\n",
    "    [5, 6, 7], # X3\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [11],\n",
    "    [27],\n",
    "    [41],\n",
    "])\n",
    "\n",
    "permute_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[0, 1, 2],\n",
       "        [3, 4, 5]]),\n",
       " array([[11],\n",
       "        [27]]))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "Batch = Tuple[ndarray, ndarray]\n",
    "\n",
    "def generate_batch(X: ndarray, y: ndarray, start: int = 0, batch_size: int = 10) -> Batch:\n",
    "    assert X.ndim == y.ndim == 2, \"X and Y must be 2 dimensional\"\n",
    "    if start+batch_size > X.shape[0]:\n",
    "        batch_size = X.shape[0] - start    \n",
    "    X_batch, y_batch = X[start:start+batch_size], y[start:start+batch_size]    \n",
    "    return X_batch, y_batch\n",
    "    \n",
    "X = np.array([\n",
    "    [0, 1, 2], # X1\n",
    "    [3, 4, 5], # X2\n",
    "    [5, 6, 7], # X3\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [11],\n",
    "    [27],\n",
    "    [41],\n",
    "])\n",
    "\n",
    "generate_batch(X, y, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n",
       "         6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02,\n",
       "         1.5300e+01, 3.9690e+02, 4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "         6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "         1.7800e+01, 3.9690e+02, 9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "         7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "         1.7800e+01, 3.9283e+02, 4.0300e+00]]),\n",
       " array([24. , 21.6, 34.7]),\n",
       " array(['CRIM', 'ZN', 'INDUS'], dtype='<U7'))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "data[0:3], target[0:3], features[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.41978194,  0.28482986, -1.2879095 , -0.27259857, -0.14421743,\n",
       "         0.41367189, -0.12001342,  0.1402136 , -0.98284286, -0.66660821,\n",
       "        -1.45900038,  0.44105193, -1.0755623 ],\n",
       "       [-0.41733926, -0.48772236, -0.59338101, -0.27259857, -0.74026221,\n",
       "         0.19427445,  0.36716642,  0.55715988, -0.8678825 , -0.98732948,\n",
       "        -0.30309415,  0.44105193, -0.49243937],\n",
       "       [-0.41734159, -0.48772236, -0.59338101, -0.27259857, -0.74026221,\n",
       "         1.28271368, -0.26581176,  0.55715988, -0.8678825 , -0.98732948,\n",
       "        -0.30309415,  0.39642699, -1.2087274 ]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[-0.41248185, -0.48772236, -1.30687771, -0.27259857, -0.83528384,\n",
       "          1.22857665, -0.51117971,  1.07773662, -0.75292215, -1.10611514,\n",
       "          0.1130321 ,  0.44105193, -1.02650148],\n",
       "        [ 5.53032093, -0.48772236,  1.01599907, -0.27259857,  0.36544404,\n",
       "         -0.75169891,  1.11749449, -1.13234958,  1.66124525,  1.53092646,\n",
       "          0.80657583, -3.88219457, -0.3564708 ],\n",
       "        [ 9.9339306 , -0.48772236,  1.01599907, -0.27259857,  1.00468047,\n",
       "          0.97356275,  0.82945385, -1.13068579,  1.66124525,  1.53092646,\n",
       "          0.80657583,  0.44105193,  0.63876307]]),\n",
       " array([[-0.41643272,  3.5896366 , -1.23392304, -0.27259857, -1.19722987,\n",
       "          2.23438568, -1.25795172,  0.62889308, -0.6379618 , -1.09423658,\n",
       "         -1.73641788,  0.39587878, -1.23956563],\n",
       "        [ 1.65868822, -0.48772236,  1.01599907, -0.27259857,  1.00468047,\n",
       "         -0.08780796,  1.11749449, -1.14513701,  1.66124525,  1.53092646,\n",
       "          0.80657583,  0.40640456,  1.27935726],\n",
       "        [ 0.21268563, -0.48772236,  1.01599907, -0.27259857,  1.36749033,\n",
       "          0.52764458,  1.05348546, -0.68445678,  1.66124525,  1.53092646,\n",
       "          0.80657583, -0.01517502,  0.71165343]]),\n",
       " array([36.2, 15. , 10.4]),\n",
       " array([48.5, 10.2, 15.2]))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "X_train[0:3], X_test[0:3], y_train[0:3], y_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[36.2],\n",
       "        [15. ],\n",
       "        [10.4]]),\n",
       " array([[48.5],\n",
       "        [10.2],\n",
       "        [15.2]]))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)\n",
    "y_train[0:3], y_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([813.2098258057695, 707.3600829629216, 419.780274960034],\n",
       " '...',\n",
       " [36.717795915470546, 14.400552796770944, 12.46628339805241],\n",
       " {'W': array([[-1.09542534],\n",
       "         [ 0.73529155],\n",
       "         [ 0.19771754],\n",
       "         [ 0.74750455],\n",
       "         [-2.28494893],\n",
       "         [ 2.20059621],\n",
       "         [ 0.7194554 ],\n",
       "         [-2.6384484 ],\n",
       "         [ 2.53576919],\n",
       "         [-1.68915096],\n",
       "         [-2.31624663],\n",
       "         [ 0.84352868],\n",
       "         [-4.25526828]]),\n",
       "  'B': array([[22.63115155]])})"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "def train(X: ndarray, \n",
    "          y: ndarray, \n",
    "          n_iter: int = 1000,\n",
    "          learning_rate: float = 0.001,\n",
    "          batch_size: int = 100,\n",
    "          return_losses: bool = True, \n",
    "          return_weights: bool = True, \n",
    "          seed: int = 1) -> None:\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    start = 0\n",
    "    weights = init_weights(X.shape[1])\n",
    "    X, y, dummy = permute_data(X, y)\n",
    "    losses = []\n",
    "    for i in range(n_iter):\n",
    "        if start >= X.shape[0]:\n",
    "            X, y, dummy = permute_data(X, y)\n",
    "            start = 0\n",
    "        X_batch, y_batch = generate_batch(X, y, start, batch_size)\n",
    "        start += batch_size\n",
    "        loss, forward_info = forward_linear_regression(X_batch, y_batch, weights)\n",
    "        if return_losses:\n",
    "            losses.append(loss)\n",
    "        loss_grads = loss_gradients(forward_info, weights)\n",
    "        for key in weights.keys():\n",
    "            weights[key] -= learning_rate * loss_grads[key]\n",
    "    \n",
    "    if return_weights:\n",
    "        return losses, weights\n",
    "\n",
    "train_info = train(X_train, y_train, n_iter = 1000, batch_size=23, seed=180708)\n",
    "losses, weights = train_info\n",
    "losses[0:3], \"...\", losses[-3:], weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}