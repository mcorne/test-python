{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Building Blocks of Neural Networks\n",
    "def assert_same_shape(array: ndarray, array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, f\"array and grad shapes do not match:  {array.shape} != {array_grad.shape}\"\n",
    "\n",
    "a, b = np.array([1, 2, 3]), np.array([[1], [2]])\n",
    "# assert_same_shape(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations\n",
    "class Operation:\n",
    "    def forward(self, input_:ndarray):\n",
    "        self.input_ = input_\n",
    "        self.output = self._output()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ParamOperation(Operation):\n",
    "    def __init__(self, param: ndarray):\n",
    "        self.param = param\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        super().backward(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "class WeightMultiply(ParamOperation):\n",
    "    def __init__(self, W: ndarray):\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return np.dot(self.input_, self.param)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.dot(output_grad, np.transpose(self.param))\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.dot(np.transpose(self.input_), output_grad)\n",
    "\n",
    "class BiasAdd(ParamOperation):\n",
    "    def __ini__(self, B: ndarray):\n",
    "        assert B.shape[0] == 1\n",
    "        super().__ini__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])\n",
    "\n",
    "class Sigmoid(Operation):\n",
    "    def _output(self) -> ndarray:\n",
    "        return 1 / (1 + np.exp(-1 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        sigmoid_backward = self._output() * (1 - self._output())\n",
    "        return sigmoid_backward * output_grad\n",
    "\n",
    "class Linear(Operation):\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Layer Blueprint\n",
    "class Layer:\n",
    "    def __init__(self, neurons: int):\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params = []\n",
    "        self.param_grads = []\n",
    "        self.operations = []\n",
    "\n",
    "    def _setup_layer(self, num_in: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "        self.input_ = input_\n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "        self.output = input_\n",
    "        return self.output        \n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "        input_grad = output_grad\n",
    "        self._param_grads()\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__,ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> ndarray: # unused!\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__.ParamOperation):\n",
    "                self.params.append(operation.param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dense Layer\n",
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int, activation: Operation = Sigmoid()) -> None:\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray):\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "        self.params = []\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons)) # weights\n",
    "        self.params.append(np.random.randn(1, self.neurons)) # bias\n",
    "        self.operations = [WeightMultiply(self.params[0]), BiasAdd(self.params[1]), self.activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Class\n",
    "class Loss:\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        assert_same_shape(prediction, target)\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        loss_value = self._output()\n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self) -> ndarray:\n",
    "        self.input_grad = self._input_grad()\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    def _output(self) -> float:\n",
    "        loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return 2 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: List[Layer], loss: Loss, seed: float = 1):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                layer.seed = self.seed\n",
    "\n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, loss_grad: ndarray):\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def train_batch(self, x_batch: ndarray, y_batch: ndarray) -> float:\n",
    "        predictions = self.forward(x_batch)\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "        self.backward(self.loss.backward())\n",
    "        return loss\n",
    "\n",
    "    def params(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "    \n",
    "    def param_grads(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "class Optimizer:\n",
    "    def __init__(self, lr: float = 0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "class SGD(Optimizer): # Stochasitc gradient descent\n",
    "    def step(self):\n",
    "        for (param, param_grad) in zip(self.net.params(), self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, net: NeuralNetwork, optim: Optimizer):\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.optim.net = net\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def generate_batches(self, X: ndarray, y: ndarray, size=32) -> Tuple[ndarray]:\n",
    "        assert X.shape[0] == y.shape[0], f\"features and targets number of rows do not match: {X.shape[0]} != {y.shape[0]}\"\n",
    "        N = X.shape[0]\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "    def fit(self, X_train: ndarray, y_train: ndarray, X_test: ndarray, y_test: ndarray, epochs=50, eval_every=10, batch_size=32, seed=1, restart=True):\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "            self.best_loss = 1e9\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            if (e+1) % eval_every == 0:\n",
    "                last_model = deepcopy(self.net)\n",
    "\n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "            batch_generator = self.generate_batches(X_train, y_train, batch_size)\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "                self.optim.step()\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    print(f\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using model from epoch {e+1-eval_every}\")\n",
    "                    self.net = last_model\n",
    "                    self.optim.net = self.net\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting Everything Together\n",
    "def mae(y_true: ndarray, y_pred: ndarray): # mean absolute error\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true: ndarray, y_pred: ndarray): # root mean squared error\n",
    "    return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n",
    "\n",
    "def eval_regression_model(model: NeuralNetwork, X_test: ndarray, y_test: ndarray):\n",
    "    preds = model.forward(X_test)\n",
    "    preds = preds.reshape(-1, 1)\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mae(preds, y_test)))\n",
    "    print()\n",
    "    print(\"Root mean squared error {:.2f}\".format(rmse(preds, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0]\n [1]\n [2]]\n[[0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "def to_2d_np(a: ndarray, type: str=\"col\") -> ndarray: # convert 1D Tensor into 2D\n",
    "    assert a.ndim == 1, \"Input tensors must be 1 dimensional\"    \n",
    "    if type == \"col\":        \n",
    "        return a.reshape(-1, 1)\n",
    "    elif type == \"row\":\n",
    "        return a.reshape(1, -1)\n",
    "\n",
    "a = np.array([0, 1, 2])\n",
    "print(to_2d_np(a))\n",
    "print(to_2d_np(a, type=\"row\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = NeuralNetwork(\n",
    "    layers=[Dense(neurons=1, activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13, activation=Sigmoid()), \n",
    "            Dense(neurons=1, activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "dl = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13, activation=Sigmoid()), \n",
    "            Dense(neurons=13, activation=Sigmoid()), \n",
    "            Dense(neurons=1,  activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation loss after 10 epochs is 30.293\nValidation loss after 20 epochs is 28.469\nValidation loss after 30 epochs is 26.293\nValidation loss after 40 epochs is 25.541\nValidation loss after 50 epochs is 25.087\n\nMean absolute error: 3.52\n\nRoot mean squared error 5.01\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(lr, SGD())\n",
    "trainer.fit(X_train, y_train, X_test, y_test, seed=20190501)\n",
    "print()\n",
    "eval_regression_model(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation loss after 10 epochs is 27.435\nValidation loss after 20 epochs is 21.839\nValidation loss after 30 epochs is 18.918\nValidation loss after 40 epochs is 17.195\nValidation loss after 50 epochs is 16.215\n\nMean absolute error: 2.60\n\nRoot mean squared error 4.03\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(nn, SGD())\n",
    "trainer.fit(X_train, y_train, X_test, y_test, seed=20190501)\n",
    "print()\n",
    "eval_regression_model(nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation loss after 10 epochs is 44.143\n",
      "Validation loss after 20 epochs is 25.278\n",
      "Validation loss after 30 epochs is 22.339\n",
      "Validation loss after 40 epochs is 16.500\n",
      "Validation loss after 50 epochs is 14.655\n",
      "\n",
      "Mean absolute error: 2.45\n",
      "\n",
      "Root mean squared error 3.83\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dl, SGD())\n",
    "trainer.fit(X_train, y_train, X_test, y_test, seed=20190501)\n",
    "print()\n",
    "eval_regression_model(dl, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}