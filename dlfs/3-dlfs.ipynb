{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Building Blocks of Neural Networks\n",
    "def assert_same_shape(array: ndarray, array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, f\"array and grad shapes do not match:  {array.shape} != {array_grad.shape}\"\n",
    "\n",
    "a, b = np.array([1, 2, 3]), np.array([[1], [2]])\n",
    "# assert_same_shape(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations\n",
    "class Operation:\n",
    "    def forward(self, input_:ndarray):\n",
    "        self.input_ = input_\n",
    "        self.output = self._output()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ParamOperation(Operation):\n",
    "    def __init__(self, param: ndarray):\n",
    "        self.param = param\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        super().backward(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "class WeightMultiply(ParamOperation):\n",
    "    def _output(self) -> ndarray:\n",
    "        return np.dot(self.input_, self.param)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.dot(output_grad, np.transpose(self.param))\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.dot(np.transpose(self.input_), output_grad)\n",
    "\n",
    "class BiasAdd(ParamOperation):\n",
    "    def __ini__(self, B: ndarray):\n",
    "        assert B.shape[0] == 1\n",
    "        super().__ini__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.one_likes(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        param_grad = np.one_likes(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])\n",
    "\n",
    "class Sigmoid(Operation):\n",
    "    def _output(self) -> ndarray:\n",
    "        return 1 / (1 + np.exp(-1 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        sigmoid_backward = self._output() * (1 - self._output())\n",
    "        return sigmoid_backward * output_grad\n",
    "\n",
    "class Linear(Operation):\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Layer Blueprint\n",
    "class Layer:\n",
    "    def __init__(self, neurons: int):\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params = []\n",
    "        self.param_grads = []\n",
    "        self.operations = []\n",
    "\n",
    "    def _setup_layer(self, num_in: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "        self.input_ = input_\n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "        self.output = input_\n",
    "        return self.output        \n",
    "    \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        for operation in reversed(self.operations)\n",
    "            output_grad = operation.backward(output_grad)\n",
    "        input_grad = output_grad\n",
    "        self._param_grads()\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        self._param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__.ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> ndarray: # unused !!!\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__.ParamOperation):\n",
    "                self.params.append(operation.param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dense Layer\n",
    "class Dense(Layer):\n",
    "    def __init__(self, neurons: int, activation: Operation = Sigmoid()) -> None:\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray):\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "        self.params = []\n",
    "        self.params.append(np.random(input_.shape[1], self.neurons)) # weights\n",
    "        self.params.append(np.random.randn(1, self.neurons)) # bias\n",
    "        self.operations = [WeightMultiply(self.params[0]), BiasAdd(self.params[1]), self.activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Class\n",
    "class Loss:\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        assert_same_shape(prediction, target)\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        loss_value = self._output()\n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self) -> ndarray:\n",
    "        self.input_grad = self._input_grad()\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    def _output(self) -> float:\n",
    "        loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return 2 * (self.prediction - self.target) / self.prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: List[Layer], loss: Loss, seed: float = 1):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                layer.seed = self.seed\n",
    "\n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, loss_grad: ndarray):\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def train_batch(self, x_batch: ndarray, y_batch: ndarray) -> float:\n",
    "        predictions = self.forward(x_batch)\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "        self.backward(self.loss.backward())\n",
    "        return loss\n",
    "\n",
    "    def params(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "    \n",
    "    def param_grads(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "class Optimizer:\n",
    "    def __init__(self, lr: float = 0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def step(self):\n",
    "        for (param, param_grad) in zip(self.net.params(), self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, net: NeuralNetwork, optim: Optimizer):\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.optim.net = net\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def generate_batches(self, X: ndarray, y: ndarray, size=632) -> Tuple[ndarray]:\n",
    "        assert X.shape[0] == y.shape[0], f\"features and targets number of rows do not match: {X.shape[0]} != {y.shape[0]}\"\n",
    "        N = X.shape[0]\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "    def fit(self, X_train: ndarray, y_train: ndarray, X_test: ndarray, epochs=100, eval_every=10, batch_size=32, seed=1, restart=True):\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            self.best_loss = 1e9\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            if (e+1) % eval_every == 0:\n",
    "                last_model = deepcopy(self.net)\n",
    "\n",
    "        X_train, y_train = permute_data(X_train, y_train)\n",
    "        batch_generator = self.generate_batches(X_train, y_train, batch_size)\n",
    "        for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "            self.net.train_batch(X_batch, y_batch)\n",
    "            self.optim.step()\n",
    "\n",
    "        if (e+1) % eval_every == 0:\n",
    "            test_preds = self.net.forward(X_test)\n",
    "            loss = self.net.loss.forward(test_preds, y_test)\n",
    "            if loss < self.best_loss:\n",
    "                print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                self.best_loss = loss\n",
    "            else:\n",
    "                print(f\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using model from epoch {e+1-eval_every}\")\n",
    "                self.net = last_model\n",
    "                self.optim.net = self.net\n",
    "                break\n",
    "\n"
   ]
  }
 ]
}